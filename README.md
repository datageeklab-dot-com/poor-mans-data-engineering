# PoorMansDataEngineering
Explore cost-effective, open-source tools and practical examples to build data pipelines, transform and store data, and optimize your data workflow without breaking the bank

## Overview

This data pipeline is designed to fetch, process, and store news sentiment data from an external API into a SQLite database. It supports two modes of operation: "backfill" and "increment." The pipeline can be scheduled to run automatically using a cron job.

## Directory Structure

The project has the following directory structure:

```bash
data-pipeline/
├── config/
│ ├── config.yaml
│ ├── timestamp.yaml
├── logs/
├── src/
│ ├── api.py
│ ├── database.py
│ ├── etl.py
├── main.py
├── requirements.txt
├── run_pipeline.sh
```

- `config/`: Contains configuration files.
- `logs/`: Stores log files generated by the pipeline.
- `src/`: Holds Python source code files.
- `venv/`: A virtual environment (optional, for package isolation).
- `main.py`: The main Python script that orchestrates the data pipeline.
- `requirements.txt`: Lists the required Python packages.
- `run_pipeline.sh`: Shell script to execute the data pipeline.


## Prerequisites

- Python 3.x
- Pip (Python package manager)
- SQLite or your own DB (for the database)

## Installation

1. Clone the repository:

```bash
git clone <repository_url>
cd data-pipeline
```

2. Create a virtual environment (optional but recommended):
```bash
python -m venv venv
source venv/bin/activate
```

3. Install required Python packages:
```bash
pip install -r requirements.txt
```

4. Configure the pipeline by editing config/config.yaml:
- Set your API key.
- Define the backfill_timestamp.
- Modify database options as needed.

## Usage

### Running the Pipeline
To run the data pipeline, execute the shell script run_pipeline.sh:
```bash
./run_pipeline.sh
```

### Cron Job
To schedule the pipeline to run automatically at specific intervals, create a crontab entry:

1. Open your crontab configuration:
```bash
crontab -e
```

2. Add a crontab entry to run the pipeline at your desired schedule. For example, to run it every day at midnight:
```bash
0 0 * * * /path/to/data-pipeline/run_pipeline.sh
```
Make sure to replace /path/to/data-pipeline with the actual path to your data pipeline directory.

## Operation Modes

- "backfill" mode: Fetches data from the API using the backfill_timestamp from the configuration file.

- "increment" mode: Fetches data from the API using the last_run_timestamp from the previous run.

The pipeline will automatically store the last_run_timestamp for the next run.

## Logging

The pipeline logs its operations in the logs/ directory, with timestamped log files.

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Make your changes.
4. Test your changes.
5. Create a pull request.

## License

This project is licensed under the MIT License.